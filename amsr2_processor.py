#!/usr/bin/env python3
"""
AMSR-2 Data Processor - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
–ü—Ä–æ—Ü–µ—Å—Å —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏–∑ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–¥–∞ + —Ñ–æ—Ä–º–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è array of dictionaries
"""

import pathlib
import datetime as dt
import tqdm
import gportal
import h5py
import numpy as np
import concurrent.futures
import threading
import time
import os
from typing import List, Tuple, Optional, Dict

from config import BASE_DIR, TEMP_DIR, GPORTAL_USERNAME, GPORTAL_PASSWORD

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ G-Portal
gportal.username = GPORTAL_USERNAME
gportal.password = GPORTAL_PASSWORD

_DS = gportal.datasets()["GCOM-W/AMSR2"]["LEVEL1"]
DS_L1B_TB = _DS["L1B-Brightness temperatureÔºàTBÔºâ"][0]


class ThreadSafeProgress:
    def __init__(self):
        self.lock = threading.Lock()
        self.downloaded = 0
        self.processed = 0
        self.deleted = 0
        self.total_files = 0

    def set_total(self, total):
        with self.lock:
            self.total_files = total

    def update_download(self):
        with self.lock:
            self.downloaded += 1
            return self.downloaded, self.total_files

    def update_processed(self):
        with self.lock:
            self.processed += 1
            return self.processed, self.total_files

    def update_deleted(self):
        with self.lock:
            self.deleted += 1
            return self.deleted, self.total_files


def calculate_lat_lon_36ghz(h5):
    lat_89 = None
    lon_89 = None

    for suffix in ["89A", "89B"]:
        lat_key = f"Latitude of Observation Point for {suffix}"
        lon_key = f"Longitude of Observation Point for {suffix}"

        if lat_key in h5 and lon_key in h5:
            lat_89 = h5[lat_key][:]
            lon_89 = h5[lon_key][:]
            break

    if lat_89 is None:
        raise ValueError("89 GHz coordinates not found in file!")

    if lat_89.shape[1] == 486:
        lat_36 = lat_89[:, ::2]
        lon_36 = lon_89[:, ::2]
    else:
        lat_36 = lat_89
        lon_36 = lon_89

    return lat_36, lon_36


def extract_swath_data(h5_path: pathlib.Path) -> Optional[Dict]:
    try:
        with h5py.File(h5_path, "r") as h5:
            var_name = "Brightness Temperature (36.5GHz,H)"
            if var_name not in h5:
                return None

            raw_temp = h5[var_name][:].astype(np.float64)

            scale = 1.0
            if "SCALE FACTOR" in h5[var_name].attrs:
                scale = h5[var_name].attrs["SCALE FACTOR"]
                if isinstance(scale, np.ndarray):
                    scale = scale[0]

            scaled_temp = np.where(raw_temp == 0, np.nan, raw_temp * scale)
            lat_36, lon_36 = calculate_lat_lon_36ghz(h5)

            if scaled_temp.shape != lat_36.shape or scaled_temp.shape != lon_36.shape:
                return None

            valid_count = np.sum(~np.isnan(scaled_temp))
            if valid_count == 0:
                return None

            orbit_type = "Unknown"
            if "A" in h5_path.stem:
                orbit_type = "Ascending"
            elif "D" in h5_path.stem:
                orbit_type = "Descending"

            metadata = {
                'filename': h5_path.name,
                'orbit_type': orbit_type,
                'shape': scaled_temp.shape,
                'scale_factor': scale,
                'valid_pixels': valid_count,
                'total_pixels': scaled_temp.size,
                'temp_range': (np.nanmin(scaled_temp), np.nanmax(scaled_temp)),
                'lat_range': (np.nanmin(lat_36), np.nanmax(lat_36)),
                'lon_range': (np.nanmin(lon_36), np.nanmax(lon_36))
            }

            return {
                'temperature': scaled_temp.astype(np.float32),
                'latitude': lat_36.astype(np.float32),
                'longitude': lon_36.astype(np.float32),
                'metadata': metadata
            }

    except Exception as e:
        print(f"Error processing {h5_path.name}: {e}")
        return None


def download_process_delete_single(product, temp_dir: pathlib.Path, progress: ThreadSafeProgress) -> Optional[Dict]:
    """
    –ö–õ–Æ–ß–ï–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –°–∫–∞—á–∏–≤–∞–µ—Ç -> –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç -> –°–†–ê–ó–£ –£–î–ê–õ–Ø–ï–¢ —Ñ–∞–π–ª
    –≠—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ!
    """
    downloaded_file = None
    try:
        # 1. –°–ö–ê–ß–ò–í–ê–ï–ú
        local_path = gportal.download(product, local_dir=str(temp_dir))
        downloaded_file = pathlib.Path(local_path)
        progress.update_download()

        # 2. –°–†–ê–ó–£ –û–ë–†–ê–ë–ê–¢–´–í–ê–ï–ú
        swath_data = extract_swath_data(downloaded_file)
        progress.update_processed()

        # 3. –ù–ï–ú–ï–î–õ–ï–ù–ù–û –£–î–ê–õ–Ø–ï–ú H5 –§–ê–ô–õ!
        try:
            downloaded_file.unlink()  # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            progress.update_deleted()
        except Exception as e:
            pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —É–¥–∞–ª–µ–Ω–∏—è

        return swath_data

    except Exception as e:
        # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –≤—Å–µ —Ä–∞–≤–Ω–æ —É–¥–∞–ª—è–µ–º —Ñ–∞–π–ª
        if downloaded_file and downloaded_file.exists():
            try:
                downloaded_file.unlink()
                progress.update_deleted()
            except:
                pass

        progress.update_download()
        progress.update_processed()
        return None


def process_files_batch_immediate_cleanup(products: List, temp_dir: pathlib.Path,
                                          max_workers: int = 4) -> List[Dict]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª—ã —Å –ù–ï–ú–ï–î–õ–ï–ù–ù–´–ú —É–¥–∞–ª–µ–Ω–∏–µ–º –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
    –ù–∏–∫–∞–∫–∏—Ö –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–π H5 —Ñ–∞–π–ª–æ–≤!
    """
    print(f"\n=== BATCH PROCESSING ({max_workers} threads) - IMMEDIATE CLEANUP ===")
    print("–§–∞–π–ª—ã —É–¥–∞–ª—è—é—Ç—Å—è –°–†–ê–ó–£ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞")

    product_list = list(products)
    total_products = len(product_list)

    progress = ThreadSafeProgress()
    progress.set_total(total_products)

    all_swaths = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏: —Å–∫–∞—á–∞—Ç—å -> –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å -> —É–¥–∞–ª–∏—Ç—å
        future_to_product = {
            executor.submit(download_process_delete_single, product, temp_dir, progress): product
            for product in product_list
        }

        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å
        with tqdm.tqdm(total=total_products, desc="Download‚ÜíProcess‚ÜíDelete") as pbar:
            for future in concurrent.futures.as_completed(future_to_product):
                result = future.result()
                if result is not None:
                    all_swaths.append(result)
                pbar.update(1)

    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(all_swaths)}/{total_products} —Ñ–∞–π–ª–æ–≤")
    print(f"H5 —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã –°–†–ê–ó–£ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ - –º–µ—Å—Ç–æ —Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ!")
    return all_swaths


def process_in_batches(products: List, temp_dir: pathlib.Path,
                       batch_size: int = 50, max_workers: int = 4) -> List[Dict]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –ü–ê–ß–ö–ê–ú–ò –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∏—Å–∫–∞
    """
    product_list = list(products)
    total_products = len(product_list)
    all_swaths = []

    print(f"\n=== BATCH PROCESSING ===")
    print(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_products}")
    print(f"–†–∞–∑–º–µ—Ä –ø–∞—á–∫–∏: {batch_size}")
    print(f"–ü–æ—Ç–æ–∫–æ–≤: {max_workers}")
    print(f"–§–∞–π–ª—ã —É–¥–∞–ª—è—é—Ç—Å—è –°–†–ê–ó–£ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∂–¥–æ–≥–æ!")

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–∞—á–∫–∏
    for batch_start in range(0, total_products, batch_size):
        batch_end = min(batch_start + batch_size, total_products)
        batch_products = product_list[batch_start:batch_end]
        batch_num = (batch_start // batch_size) + 1
        total_batches = (total_products + batch_size - 1) // batch_size

        print(f"\n--- –ü–ê–ß–ö–ê {batch_num}/{total_batches} ({len(batch_products)} —Ñ–∞–π–ª–æ–≤) ---")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—á–∫—É —Å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–º —É–¥–∞–ª–µ–Ω–∏–µ–º
        batch_swaths = process_files_batch_immediate_cleanup(
            batch_products, temp_dir, max_workers
        )

        all_swaths.extend(batch_swaths)

        print(f"–ü–∞—á–∫–∞ {batch_num} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ü–æ–ª—É—á–µ–Ω–æ {len(batch_swaths)} —Å–≤–æ—Ç–æ–≤.")
        print(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(all_swaths)}/{total_products}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞
        disk_usage = get_disk_usage(temp_dir)
        print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞: {disk_usage:.1f} MB")

    return all_swaths


def get_disk_usage(directory: pathlib.Path) -> float:
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞ –≤ –ú–ë"""
    try:
        total_size = 0
        if directory.exists():
            for file_path in directory.rglob('*'):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
        return total_size / (1024 * 1024)
    except:
        return 0.0


def save_swaths_array(swath_list: List[Dict], base_dir: pathlib.Path,
                      period_name: str, start_datetime: str, end_datetime: str) -> pathlib.Path:
    """
    Save as array of dictionaries (original structure)
    """
    print(f"\n=== SAVING ARRAY OF SWATH DICTIONARIES ===")

    output_file = base_dir / f"AMSR2_swaths_{period_name}.npz"

    print("Preparing swath array structure...")

    # Create array of dictionaries structure
    swath_array = []
    for i, swath in enumerate(tqdm.tqdm(swath_list, desc="Preparing swath array")):
        swath_dict = {
            'temperature': swath['temperature'].astype(np.float32),
            'latitude': swath['latitude'].astype(np.float32),
            'longitude': swath['longitude'].astype(np.float32),
            'metadata': swath['metadata']
        }
        swath_array.append(swath_dict)

    # Save as compressed NPZ
    save_dict = {
        'swath_array': swath_array,
        'period': f"{start_datetime} to {end_datetime}",
        'num_swaths': len(swath_list),
        'description': 'AMSR-2 36.5GHz H swath data - Array of dictionaries format'
    }

    print("Saving with maximum compression...")
    np.savez_compressed(output_file, **save_dict)

    # Statistics
    file_size_mb = output_file.stat().st_size / (1024 * 1024)
    total_pixels = sum(s['metadata']['valid_pixels'] for s in swath_list)

    print(f"NPZ file saved: {output_file.name}")
    print(f"File size: {file_size_mb:.2f} MB")
    print(f"Total pixels: {total_pixels:,}")
    print(f"Structure: Array of {len(swath_list)} swath dictionaries")

    return output_file


def load_swath_array(dataset_path: pathlib.Path) -> List[Dict]:
    """
    Load data from NPZ file
    """
    with np.load(dataset_path, allow_pickle=True) as data:
        swath_array = data['swath_array']
        num_swaths = int(data['num_swaths'])

        print(f"Loaded array of {num_swaths} swath dictionaries from NPZ file")
        return swath_array.tolist()


def get_optimal_settings():
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏—Å—Ç–µ–º—ã"""
    cpu_count = os.cpu_count() or 4

    # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥
    max_workers = min(cpu_count, 6)  # –ù–µ –±–æ–ª–µ–µ 6 –ø–æ—Ç–æ–∫–æ–≤
    batch_size = 30  # –ù–µ–±–æ–ª—å—à–∏–µ –ø–∞—á–∫–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞

    return max_workers, batch_size


def ask_compression_level():
    """–°–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ –∂–µ–ª–∞–µ–º–æ–º —É—Ä–æ–≤–Ω–µ —Å–∂–∞—Ç–∏—è"""
    print(f"\n–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∂–∞—Ç–∏—è —Ñ–∞–π–ª–∞:")
    print("1. –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ (–±—ã—Å—Ç—Ä–æ, –±–æ–ª—å—à–µ —Ä–∞–∑–º–µ—Ä)")
    print("2. –°—Ä–µ–¥–Ω–µ–µ —Å–∂–∞—Ç–∏–µ (—É–±–∏—Ä–∞–µ—Ç –ø—É—Å—Ç—ã–µ –æ–±–ª–∞—Å—Ç–∏, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)")
    print("3. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ (–º–µ–¥–ª–µ–Ω–Ω–æ, –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä)")

    while True:
        try:
            level = input("–í—ã–±–µ—Ä–∏—Ç–µ —É—Ä–æ–≤–µ–Ω—å —Å–∂–∞—Ç–∏—è (1-3, Enter –¥–ª—è 2): ").strip()

            if level == "":
                return 2

            level = int(level)
            if 1 <= level <= 3:
                descriptions = {
                    1: "–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ",
                    2: "—Å—Ä–µ–¥–Ω–µ–µ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)",
                    3: "–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ"
                }
                print(f"–í—ã–±—Ä–∞–Ω–æ: {descriptions[level]} —Å–∂–∞—Ç–∏–µ")
                return level
            else:
                print("–í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ –æ—Ç 1 –¥–æ 3")

        except ValueError:
            print("–í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ –æ—Ç 1 –¥–æ 3")
        except KeyboardInterrupt:
            return 2


def fetch_amsr2_data(start_datetime: str, end_datetime: str,
                     base: pathlib.Path = BASE_DIR,
                     temp_dir: Optional[pathlib.Path] = None,
                     max_workers: Optional[int] = None,
                     batch_size: Optional[int] = None,
                     compression_level: int = 2) -> pathlib.Path:
    """
    –û–ë–™–ï–î–ò–ù–ï–ù–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è:
    - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–º —É–¥–∞–ª–µ–Ω–∏–µ–º
    - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ array of dictionaries —Å –∫–æ–º–ø—Ä–µ—Å—Å–∏–µ–π
    """

    # –ü–æ–ª—É—á–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    if max_workers is None or batch_size is None:
        opt_workers, opt_batch = get_optimal_settings()
        max_workers = max_workers or opt_workers
        batch_size = batch_size or opt_batch

    print(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∏: {max_workers} –ø–æ—Ç–æ–∫–æ–≤, –ø–∞—á–∫–∏ –ø–æ {batch_size} —Ñ–∞–π–ª–æ–≤")
    print(f"–í–ê–ñ–ù–û: H5 —Ñ–∞–π–ª—ã —É–¥–∞–ª—è—é—Ç—Å—è –°–†–ê–ó–£ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏!")

    base.mkdir(parents=True, exist_ok=True)

    if temp_dir is None:
        temp_dir = TEMP_DIR
    temp_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n=== SEARCHING FOR AMSR-2 DATA ===")
    print(f"–ü–µ—Ä–∏–æ–¥: {start_datetime} ‚Üí {end_datetime}")

    search_start = time.time()
    res = gportal.search(
        dataset_ids=[DS_L1B_TB],
        start_time=start_datetime,
        end_time=end_datetime
    )
    search_time = time.time() - search_start

    total_files = res.matched()
    print(f"–ù–∞–π–¥–µ–Ω–æ {total_files} —Ñ–∞–π–ª–æ–≤ –∑–∞ {search_time:.1f} —Å–µ–∫—É–Ω–¥")

    if total_files == 0:
        print("–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return None

    # –û—Ü–µ–Ω–∫–∞ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ
    estimated_size_mb = total_files * 30  # ~30 –ú–ë –Ω–∞ —Ñ–∞–π–ª
    batch_size_mb = batch_size * 30

    print(f"–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –≤—Å–µ—Ö H5: ~{estimated_size_mb:.0f} –ú–ë")
    print(f"–ú–∞–∫—Å–∏–º—É–º –Ω–∞ –¥–∏—Å–∫–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ: ~{batch_size_mb:.0f} –ú–ë (–ø–∞—á–∫–∞)")
    print(f"–≠–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞: {(estimated_size_mb - batch_size_mb) / estimated_size_mb * 100:.0f}%")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–º —É–¥–∞–ª–µ–Ω–∏–µ–º
    processing_start = time.time()
    all_products = res.products()
    all_swaths = process_in_batches(all_products, temp_dir, batch_size, max_workers)
    processing_time = time.time() - processing_start

    print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(all_swaths)} —Å–≤–æ—Ç–æ–≤")

    if not all_swaths:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ")
        return None

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_pixels = sum(s['metadata']['valid_pixels'] for s in all_swaths)
    print(f"–í—Å–µ–≥–æ –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–∏–∫—Å–µ–ª–µ–π: {total_pixels:,}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ array of dictionaries —Å –∫–æ–º–ø—Ä–µ—Å—Å–∏–µ–π
    period_name = f"{start_datetime.replace(':', '').replace('-', '').replace('T', '_')}_to_{end_datetime.replace(':', '').replace('-', '').replace('T', '_')}"

    save_start = time.time()
    output_file = save_swaths_array(all_swaths, base, period_name, start_datetime, end_datetime)
    save_time = time.time() - save_start

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–ø–∫–∏
    try:
        temp_dir.rmdir()
        print(f"–í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞ —É–¥–∞–ª–µ–Ω–∞: {temp_dir}")
    except:
        print(f"–í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞ –Ω–µ –ø—É—Å—Ç–∞: {temp_dir}")

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_time = search_time + processing_time + save_time
    print(f"\n=== –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===")
    print(f"–ü–æ–∏—Å–∫: {search_time:.1f}—Å | –û–±—Ä–∞–±–æ—Ç–∫–∞: {processing_time:.1f}—Å | –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {save_time:.1f}—Å")
    print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f}—Å ({total_time / 60:.1f} –º–∏–Ω—É—Ç)")
    print(f"–°–∫–æ—Ä–æ—Å—Ç—å: {total_files / total_time:.2f} —Ñ–∞–π–ª–æ–≤/—Å–µ–∫")
    print(f"‚úÖ H5 —Ñ–∞–π–ª—ã –ù–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã - –º–µ—Å—Ç–æ —Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ!")

    return output_file


def main():
    """
    –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∑–∞–ø—É—Å–∫ —Å –≤–≤–æ–¥–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    """
    print("=== AMSR-2 PROCESSOR ===")
    print("–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:")
    print("‚Ä¢ H5 —Ñ–∞–π–ª—ã —É–¥–∞–ª—è—é—Ç—Å—è –°–†–ê–ó–£ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    print("‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—á–∫–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞")
    print("‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ array of dictionaries —Å –∫–æ–º–ø—Ä–µ—Å—Å–∏–µ–π")
    print("‚Ä¢ –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏")

    # –í–≤–æ–¥ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
    print("\n–í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª:")
    print("–§–æ—Ä–º–∞—Ç: YYYY-MM-DD HH:MM:SS (–Ω–∞–ø—Ä–∏–º–µ—Ä, 2025-05-20 14:30:00)")

    while True:
        try:
            start_input = input("–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: ").strip()
            end_input = input("–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è: ").strip()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç
            dt_start = dt.datetime.strptime(start_input, "%Y-%m-%d %H:%M:%S")
            dt_end = dt.datetime.strptime(end_input, "%Y-%m-%d %H:%M:%S")

            if dt_end <= dt_start:
                print("–û—à–∏–±–∫–∞: –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–∑–∂–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞.")
                continue

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ ISO —Ñ–æ—Ä–º–∞—Ç
            start_datetime = dt_start.isoformat()
            end_datetime = dt_end.isoformat()

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            duration = dt_end - dt_start
            hours = duration.total_seconds() / 3600
            print(f"–í—ã–±—Ä–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥: {start_input} ‚Üí {end_input} ({hours:.1f} —á–∞—Å–æ–≤)")

            # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            est_files = hours * 15  # –ø—Ä–∏–º–µ—Ä–Ω–æ 15 —Ñ–∞–π–ª–æ–≤ –≤ —á–∞—Å
            est_size_gb = est_files * 30 / 1024  # H5 —Ä–∞–∑–º–µ—Ä
            final_size_mb = est_files * 0.5  # NPZ —Å–∂–∞—Ç—ã–π —Ä–∞–∑–º–µ—Ä
            print(f"–û–∂–∏–¥–∞–µ–º–æ —Ñ–∞–π–ª–æ–≤: ~{est_files:.0f}")
            print(f"H5 —Ä–∞–∑–º–µ—Ä (–ù–ï —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è): ~{est_size_gb:.1f} –ì–ë")
            print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π NPZ: ~{final_size_mb:.0f} –ú–ë")
            print(f"–≠–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞: {est_size_gb * 1024 / final_size_mb:.1f}x!")

            break

        except ValueError:
            print("–û—à–∏–±–∫–∞: –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ YYYY-MM-DD HH:MM:SS")
        except KeyboardInterrupt:
            print("\n–û—Ç–º–µ–Ω–µ–Ω–æ.")
            return

    # –í–≤–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ—Ç–æ–∫–æ–≤ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫
    print(f"\n–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    opt_workers, opt_batch = get_optimal_settings()
    print(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:")
    print(f"  –ü–æ—Ç–æ–∫–æ–≤: {opt_workers}")
    print(f"  –†–∞–∑–º–µ—Ä –ø–∞—á–∫–∏: {opt_batch} —Ñ–∞–π–ª–æ–≤")

    use_custom = input("–ò–∑–º–µ–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏? (y/n): ").strip().lower()

    if use_custom == 'y':
        try:
            workers = int(input(f"–ü–æ—Ç–æ–∫–æ–≤ (1-8, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è {opt_workers}): ").strip())
            batch = int(input(f"–†–∞–∑–º–µ—Ä –ø–∞—á–∫–∏ (10-100, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è {opt_batch}): ").strip())

            max_workers = max(1, min(8, workers))
            batch_size = max(10, min(100, batch))
        except ValueError:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
            max_workers, batch_size = opt_workers, opt_batch
    else:
        max_workers, batch_size = opt_workers, opt_batch

    # –ó–∞–ø—Ä–æ—Å —É—Ä–æ–≤–Ω—è —Å–∂–∞—Ç–∏—è
    compression_level = ask_compression_level()

    # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    print(f"\n=== –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–ò–ï ===")
    print(f"–ü–µ—Ä–∏–æ–¥: {start_input} ‚Üí {end_input}")
    print(f"–ü–æ—Ç–æ–∫–æ–≤: {max_workers}")
    print(f"–†–∞–∑–º–µ—Ä –ø–∞—á–∫–∏: {batch_size}")
    print(f"–°–∂–∞—Ç–∏–µ: —É—Ä–æ–≤–µ–Ω—å {compression_level}")
    print(f"–û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: ~{est_files / max_workers / 60:.1f} –º–∏–Ω—É—Ç")

    confirm = input("\n–ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É? (y/n): ").strip().lower()
    if confirm not in ['y', 'yes', '–¥–∞', '–¥']:
        print("–û—Ç–º–µ–Ω–µ–Ω–æ.")
        return

    print(f"\n=== –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–†–ê–ë–û–¢–ö–£ ===")
    overall_start = time.time()

    output_file = fetch_amsr2_data(
        start_datetime, end_datetime,
        max_workers=max_workers,
        batch_size=batch_size,
        compression_level=compression_level
    )

    overall_time = time.time() - overall_start

    if output_file:
        print(f"\n=== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–ì–†–£–ó–ö–ò ===")

        load_start = time.time()
        swath_array = load_swath_array(output_file)
        load_time = time.time() - load_start

        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(swath_array)} —Å–≤–æ—Ç–æ–≤ –∑–∞ {load_time:.1f} —Å–µ–∫—É–Ω–¥")

        if len(swath_array) > 0:
            example_swath = swath_array[0]
            temp_array = example_swath['temperature']
            lat_array = example_swath['latitude']
            lon_array = example_swath['longitude']

            print(f"\n–ü—Ä–∏–º–µ—Ä —Å–≤–æ—Ç–∞:")
            print(f"  –†–∞–∑–º–µ—Ä —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä: {temp_array.shape}")
            print(f"  –î–∏–∞–ø–∞–∑–æ–Ω —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä: {np.nanmin(temp_array):.1f} - {np.nanmax(temp_array):.1f} K")
            print(f"  –î–∏–∞–ø–∞–∑–æ–Ω —à–∏—Ä–æ—Ç: {np.nanmin(lat_array):.1f}¬∞ - {np.nanmax(lat_array):.1f}¬∞")
            print(f"  –î–∏–∞–ø–∞–∑–æ–Ω –¥–æ–ª–≥–æ—Ç: {np.nanmin(lon_array):.1f}¬∞ - {np.nanmax(lon_array):.1f}¬∞")

    print(f"\n=== –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê ===")
    print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {overall_time:.1f} —Å–µ–∫—É–Ω–¥ ({overall_time / 60:.1f} –º–∏–Ω—É—Ç)")
    print("üóëÔ∏è H5 —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã - –º–µ—Å—Ç–æ —Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ!")
    print("üíæ –°–∂–∞—Ç—ã–π NPZ —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ array of dictionaries —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")


if __name__ == "__main__":
    main()
    # !/usr/bin/env python3
"""
AMSR-2 Data Processor - –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è PyCharm
–ü—Ä—è–º–æ–π –ø–µ—Ä–µ–Ω–æ—Å –≤–∞—à–µ–≥–æ –∫–æ–¥–∞ –∏–∑ Google Colab
"""

import pathlib
import datetime as dt
import tqdm
import gportal
import h5py
import numpy as np
import concurrent.futures
import threading
import time
import os
from typing import List, Tuple, Optional, Dict

from config import BASE_DIR, TEMP_DIR, GPORTAL_USERNAME, GPORTAL_PASSWORD

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ G-Portal
gportal.username = GPORTAL_USERNAME
gportal.password = GPORTAL_PASSWORD

_DS = gportal.datasets()["GCOM-W/AMSR2"]["LEVEL1"]
DS_L1B_TB = _DS["L1B-Brightness temperatureÔºàTBÔºâ"][0]


class ThreadSafeProgress:
    def __init__(self):
        self.lock = threading.Lock()
        self.processed = 0
        self.total_files = 0

    def set_total(self, total):
        with self.lock:
            self.total_files = total

    def update_processed(self):
        with self.lock:
            self.processed += 1
            return self.processed, self.total_files


def calculate_lat_lon_36ghz(h5):
    lat_89 = None
    lon_89 = None

    for suffix in ["89A", "89B"]:
        lat_key = f"Latitude of Observation Point for {suffix}"
        lon_key = f"Longitude of Observation Point for {suffix}"

        if lat_key in h5 and lon_key in h5:
            lat_89 = h5[lat_key][:]
            lon_89 = h5[lon_key][:]
            break

    if lat_89 is None:
        raise ValueError("89 GHz coordinates not found in file!")

    if lat_89.shape[1] == 486:
        lat_36 = lat_89[:, ::2]
        lon_36 = lon_89[:, ::2]
    else:
        lat_36 = lat_89
        lon_36 = lon_89

    return lat_36, lon_36


def extract_swath_data(h5_path: pathlib.Path) -> Optional[Dict]:
    try:
        with h5py.File(h5_path, "r") as h5:
            var_name = "Brightness Temperature (36.5GHz,H)"
            if var_name not in h5:
                return None

            raw_temp = h5[var_name][:].astype(np.float64)

            scale = 1.0
            if "SCALE FACTOR" in h5[var_name].attrs:
                scale = h5[var_name].attrs["SCALE FACTOR"]
                if isinstance(scale, np.ndarray):
                    scale = scale[0]

            scaled_temp = np.where(raw_temp == 0, np.nan, raw_temp * scale)
            lat_36, lon_36 = calculate_lat_lon_36ghz(h5)

            if scaled_temp.shape != lat_36.shape or scaled_temp.shape != lon_36.shape:
                return None

            valid_count = np.sum(~np.isnan(scaled_temp))
            if valid_count == 0:
                return None

            orbit_type = "Unknown"
            if "A" in h5_path.stem:
                orbit_type = "Ascending"
            elif "D" in h5_path.stem:
                orbit_type = "Descending"

            metadata = {
                'filename': h5_path.name,
                'orbit_type': orbit_type,
                'shape': scaled_temp.shape,
                'scale_factor': scale,
                'valid_pixels': valid_count,
                'total_pixels': scaled_temp.size,
                'temp_range': (np.nanmin(scaled_temp), np.nanmax(scaled_temp)),
                'lat_range': (np.nanmin(lat_36), np.nanmax(lat_36)),
                'lon_range': (np.nanmin(lon_36), np.nanmax(lon_36))
            }

            return {
                'temperature': scaled_temp.astype(np.float32),
                'latitude': lat_36.astype(np.float32),
                'longitude': lon_36.astype(np.float32),
                'metadata': metadata
            }

    except Exception as e:
        print(f"Error processing {h5_path.name}: {e}")
        return None


def download_and_process_single(product, temp_dir: pathlib.Path, progress: ThreadSafeProgress) -> Optional[Dict]:
    """
    Download -> Process -> Delete file immediately
    """
    downloaded_file = None
    try:
        # Download
        local_path = gportal.download(product, local_dir=str(temp_dir))
        downloaded_file = pathlib.Path(local_path)

        # Process immediately
        swath_data = extract_swath_data(downloaded_file)

        # Delete file immediately
        try:
            downloaded_file.unlink()
        except Exception as e:
            pass

        progress.update_processed()
        return swath_data

    except Exception as e:
        if downloaded_file and downloaded_file.exists():
            try:
                downloaded_file.unlink()
            except:
                pass

        progress.update_processed()
        return None


def process_files_concurrent(products: List, temp_dir: pathlib.Path,
                             max_workers: int = 4) -> List[Dict]:
    """
    Process files with immediate cleanup using concurrent threads
    """
    print(f"\n=== CONCURRENT PROCESSING ({max_workers} threads) ===")

    product_list = list(products)
    total_products = len(product_list)

    progress = ThreadSafeProgress()
    progress.set_total(total_products)

    all_swaths = []
    swaths_lock = threading.Lock()

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_product = {
            executor.submit(download_and_process_single, product, temp_dir, progress): product
            for product in product_list
        }

        with tqdm.tqdm(total=total_products, desc="Processing files") as pbar:
            for future in concurrent.futures.as_completed(future_to_product):
                result = future.result()
                if result is not None:
                    with swaths_lock:
                        all_swaths.append(result)
                pbar.update(1)

    print(f"Successfully processed: {len(all_swaths)}/{total_products} files")
    return all_swaths


def save_swaths_array(swath_list: List[Dict], base_dir: pathlib.Path,
                      period_name: str, start_datetime: str, end_datetime: str) -> pathlib.Path:
    """
    Save as array of dictionaries (new structure)
    """
    print(f"\n=== SAVING ARRAY OF SWATH DICTIONARIES ===")

    output_file = base_dir / f"AMSR2_swaths_{period_name}.npz"

    print("Preparing swath array structure...")

    # Create array of dictionaries structure
    swath_array = []
    for i, swath in enumerate(tqdm.tqdm(swath_list, desc="Preparing swath array")):
        swath_dict = {
            'temperature': swath['temperature'].astype(np.float32),
            'latitude': swath['latitude'].astype(np.float32),
            'longitude': swath['longitude'].astype(np.float32),
            'metadata': swath['metadata']
        }
        swath_array.append(swath_dict)

    # Save as compressed NPZ
    save_dict = {
        'swath_array': swath_array,
        'period': f"{start_datetime} to {end_datetime}",
        'num_swaths': len(swath_list),
        'description': 'AMSR-2 36.5GHz H swath data - Array of dictionaries format'
    }

    print("Saving with maximum compression...")
    np.savez_compressed(output_file, **save_dict)

    # Statistics
    file_size_mb = output_file.stat().st_size / (1024 * 1024)
    total_pixels = sum(s['metadata']['valid_pixels'] for s in swath_list)

    print(f"NPZ file saved: {output_file.name}")
    print(f"File size: {file_size_mb:.2f} MB")
    print(f"Total pixels: {total_pixels:,}")
    print(f"Structure: Array of {len(swath_list)} swath dictionaries")

    return output_file


def get_optimal_settings():
    """
    Determine optimal settings based on system resources
    """
    cpu_count = os.cpu_count() or 4
    max_workers = min(cpu_count, 8)
    return max_workers


def fetch_amsr2_data(start_datetime: str, end_datetime: str,
                     base: pathlib.Path = BASE_DIR,
                     temp_dir: Optional[pathlib.Path] = None,
                     max_workers: Optional[int] = None,
                     compression_level: int = 2) -> pathlib.Path:
    """
    Main function
    """

    if max_workers is None:
        max_workers = get_optimal_settings()

    print(f"Settings: {max_workers} threads")

    base.mkdir(parents=True, exist_ok=True)

    if temp_dir is None:
        temp_dir = TEMP_DIR
    temp_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n=== SEARCHING FOR AMSR-2 DATA ===")
    print(f"Period: {start_datetime} ‚Üí {end_datetime}")

    search_start = time.time()
    res = gportal.search(
        dataset_ids=[DS_L1B_TB],
        start_time=start_datetime,
        end_time=end_datetime
    )
    search_time = time.time() - search_start

    total_files = res.matched()
    print(f"Found {total_files} files in {search_time:.1f} seconds")

    if total_files == 0:
        print("No data found")
        return None

    # Processing with immediate cleanup
    processing_start = time.time()
    all_products = res.products()
    all_swaths = process_files_concurrent(all_products, temp_dir, max_workers)
    processing_time = time.time() - processing_start

    print(f"\nProcessing completed in {processing_time:.1f} seconds")
    print(f"Extracted {len(all_swaths)} swaths")

    if not all_swaths:
        print("Failed to process data")
        return None

    # Statistics
    total_pixels = sum(s['metadata']['valid_pixels'] for s in all_swaths)
    print(f"Total valid pixels: {total_pixels:,}")

    # Save
    period_name = f"{start_datetime.replace(':', '').replace('-', '').replace('T', '_')}_to_{end_datetime.replace(':', '').replace('-', '').replace('T', '_')}"

    save_start = time.time()
    output_file = save_swaths_array(all_swaths, base, period_name, start_datetime, end_datetime, compression_level)
    save_time = time.time() - save_start

    # Cleanup temp directory
    try:
        temp_dir.rmdir()
        print(f"Temp directory removed: {temp_dir}")
    except:
        print(f"Temp directory not empty: {temp_dir}")

    # Final statistics
    total_time = search_time + processing_time + save_time
    print(f"\n=== FINAL STATISTICS ===")
    print(f"Search: {search_time:.1f}s | Processing: {processing_time:.1f}s | Saving: {save_time:.1f}s")
    print(f"Total time: {total_time:.1f}s ({total_time / 60:.1f} minutes)")
    print(f"Speed: {total_files / total_time:.2f} files/sec")

    return output_file


def load_swath_array(dataset_path: pathlib.Path) -> List[Dict]:
    """
    Load data from NPZ file
    """
    with np.load(dataset_path, allow_pickle=True) as data:
        swath_array = data['swath_array']
        num_swaths = int(data['num_swaths'])

        print(f"Loaded array of {num_swaths} swath dictionaries from NPZ file")
        return swath_array.tolist()


def main():
    """
    –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∑–∞–ø—É—Å–∫ —Å –≤–≤–æ–¥–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    """
    print("=== AMSR-2 PROCESSOR ===")
    print("–í–≤–µ–¥–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö AMSR-2:")

    # –í–≤–æ–¥ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
    print("\n–í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª:")
    print("–§–æ—Ä–º–∞—Ç: YYYY-MM-DD HH:MM:SS (–Ω–∞–ø—Ä–∏–º–µ—Ä, 2025-05-20 14:30:00)")

    while True:
        try:
            start_input = input("–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: ").strip()
            end_input = input("–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è: ").strip()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç
            dt_start = dt.datetime.strptime(start_input, "%Y-%m-%d %H:%M:%S")
            dt_end = dt.datetime.strptime(end_input, "%Y-%m-%d %H:%M:%S")

            if dt_end <= dt_start:
                print("–û—à–∏–±–∫–∞: –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–∑–∂–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞.")
                continue

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ ISO —Ñ–æ—Ä–º–∞—Ç
            start_datetime = dt_start.isoformat()
            end_datetime = dt_end.isoformat()

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            duration = dt_end - dt_start
            hours = duration.total_seconds() / 3600
            print(f"–í—ã–±—Ä–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥: {start_input} ‚Üí {end_input} ({hours:.1f} —á–∞—Å–æ–≤)")

            # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            est_files = hours * 15  # –ø—Ä–∏–º–µ—Ä–Ω–æ 15 —Ñ–∞–π–ª–æ–≤ –≤ —á–∞—Å
            est_size_mb = est_files * 0.5  # –ø—Ä–∏–º–µ—Ä–Ω–æ 0.5 –ú–ë –Ω–∞ —Ñ–∞–π–ª
            print(f"–û–∂–∏–¥–∞–µ–º–æ —Ñ–∞–π–ª–æ–≤: ~{est_files:.0f}")
            print(f"–†–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: ~{est_size_mb:.0f} –ú–ë")

            break

        except ValueError:
            print("–û—à–∏–±–∫–∞: –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ YYYY-MM-DD HH:MM:SS")
        except KeyboardInterrupt:
            print("\n–û—Ç–º–µ–Ω–µ–Ω–æ.")
            return

    # –í–≤–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ—Ç–æ–∫–æ–≤
    print(f"\n–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ—Ç–æ–∫–æ–≤:")
    optimal_workers = get_optimal_settings()
    print(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤: {optimal_workers}")

    while True:
        try:
            workers_input = input(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ (1-16, Enter –¥–ª—è {optimal_workers}): ").strip()

            if workers_input == "":
                max_workers = optimal_workers
            else:
                max_workers = int(workers_input)
                max_workers = max(1, min(16, max_workers))  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 1-16

            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Ç–æ–∫–æ–≤: {max_workers}")
            break

        except ValueError:
            print("–û—à–∏–±–∫–∞: –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ –æ—Ç 1 –¥–æ 16")
        except KeyboardInterrupt:
            print("\n–û—Ç–º–µ–Ω–µ–Ω–æ.")
            return

    # –ó–∞–ø—Ä–æ—Å —É—Ä–æ–≤–Ω—è —Å–∂–∞—Ç–∏—è
    compression_level = ask_compression_level()

    # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    print(f"\n=== –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–ò–ï ===")
    print(f"–ü–µ—Ä–∏–æ–¥: {start_input} ‚Üí {end_input}")
    print(f"–ü–æ—Ç–æ–∫–æ–≤: {max_workers}")
    print(f"–°–∂–∞—Ç–∏–µ: —É—Ä–æ–≤–µ–Ω—å {compression_level}")
    print(f"–û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: ~{est_files / max_workers / 60:.1f} –º–∏–Ω—É—Ç")

    confirm = input("\n–ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É? (y/n): ").strip().lower()
    if confirm not in ['y', 'yes', '–¥–∞', '–¥']:
        print("–û—Ç–º–µ–Ω–µ–Ω–æ.")
        return

    print(f"\n=== –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–†–ê–ë–û–¢–ö–£ ===")
    overall_start = time.time()

    output_file = fetch_amsr2_data(
        start_datetime, end_datetime,
        max_workers=max_workers,
        compression_level=compression_level
    )

    overall_time = time.time() - overall_start

    if output_file:
        print(f"\n=== TESTING LOAD ===")

        load_start = time.time()
        swath_array = load_swath_array(output_file)
        load_time = time.time() - load_start

        print(f"Loaded {len(swath_array)} swaths in {load_time:.1f} seconds")

        if len(swath_array) > 0:
            example_swath = swath_array[0]
            temp_array = example_swath['temperature']
            lat_array = example_swath['latitude']
            lon_array = example_swath['longitude']

            print(f"\nExample swath:")
            print(f"  Temperature shape: {temp_array.shape}")
            print(f"  Temperature range: {np.nanmin(temp_array):.1f} - {np.nanmax(temp_array):.1f} K")
            print(f"  Latitude range: {np.nanmin(lat_array):.1f}¬∞ - {np.nanmax(lat_array):.1f}¬∞")
            print(f"  Longitude range: {np.nanmin(lon_array):.1f}¬∞ - {np.nanmax(lon_array):.1f}¬∞")

    print(f"\n=== PROCESSING COMPLETED ===")
    print(f"Total time: {overall_time:.1f} seconds ({overall_time / 60:.1f} minutes)")


if __name__ == "__main__":
    main()