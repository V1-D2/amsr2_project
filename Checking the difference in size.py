#!/usr/bin/env python3
"""
AMSR2 Swath Size Checker
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–∑–º–µ—Ä—ã swaths –Ω–∞–ø—Ä—è–º—É—é –∏–∑ gportal –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è
–ø–æ—á–µ–º—É —Ä–∞–∑–º–µ—Ä—ã —Ä–∞–∑–Ω—ã–µ
"""

import pathlib
import datetime as dt
import gportal
import h5py
import numpy as np
from typing import List, Dict, Optional

# Import your config
from config import BASE_DIR, TEMP_DIR, GPORTAL_USERNAME, GPORTAL_PASSWORD

# Setup gportal
gportal.username = GPORTAL_USERNAME
gportal.password = GPORTAL_PASSWORD

_DS = gportal.datasets()["GCOM-W/AMSR2"]["LEVEL1"]
DS_L1B_TB = _DS["L1B-Brightness temperatureÔºàTBÔºâ"][0]


def check_h5_file_info(h5_path: pathlib.Path) -> Optional[Dict]:
    """
    –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ H5 —Ñ–∞–π–ª–µ –≤–∫–ª—é—á–∞—è –≤—Å–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –∏—Ö —Ä–∞–∑–º–µ—Ä—ã
    """
    try:
        with h5py.File(h5_path, "r") as h5:
            info = {
                'filename': h5_path.name,
                'variables': {},
                'file_size_mb': h5_path.stat().st_size / (1024 * 1024)
            }

            # –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            def collect_info(name, obj):
                if isinstance(obj, h5py.Dataset):
                    info['variables'][name] = {
                        'shape': obj.shape,
                        'dtype': str(obj.dtype),
                        'size_mb': obj.nbytes / (1024 * 1024)
                    }

                    # –î–ª—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—É—á–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ
                    if "Brightness Temperature" in name:
                        # Get scale factor
                        scale = 1.0
                        if "SCALE FACTOR" in obj.attrs:
                            scale = obj.attrs["SCALE FACTOR"]
                            if isinstance(scale, np.ndarray):
                                scale = scale[0]

                        data = obj[:]
                        valid_mask = data != 0
                        valid_count = np.sum(valid_mask)

                        info['variables'][name].update({
                            'scale_factor': float(scale),
                            'valid_pixels': int(valid_count),
                            'total_pixels': int(data.size),
                            'valid_percentage': float(valid_count / data.size * 100),
                            'data_range': (int(np.min(data[valid_mask])) if valid_count > 0 else 0,
                                           int(np.max(data[valid_mask])) if valid_count > 0 else 0)
                        })

            h5.visititems(collect_info)
            return info

    except Exception as e:
        print(f"‚ùå Error reading {h5_path.name}: {e}")
        return None


def download_and_check_few_files(start_datetime: str, end_datetime: str, max_files: int = 5) -> List[Dict]:
    """
    –°–∫–∞—á–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Ö —Ä–∞–∑–º–µ—Ä—ã
    """
    print(f"üîç Searching for AMSR-2 data: {start_datetime} ‚Üí {end_datetime}")

    # Create temp directory
    TEMP_DIR.mkdir(parents=True, exist_ok=True)

    # Search for data
    res = gportal.search(
        dataset_ids=[DS_L1B_TB],
        start_time=start_datetime,
        end_time=end_datetime
    )

    total_files = res.matched()
    print(f"üìä Found {total_files} files total")

    if total_files == 0:
        print("‚ùå No data found")
        return []

    # Get first few products
    all_products = res.products()
    products_to_check = list(all_products)[:max_files]

    print(f"üì• Will download and check first {len(products_to_check)} files")

    results = []

    for i, product in enumerate(products_to_check):
        try:
            print(f"\nüì¶ Processing file {i + 1}/{len(products_to_check)}")

            # Download file
            print(f"‚¨áÔ∏è  Downloading...")
            local_path = gportal.download(product, local_dir=str(TEMP_DIR))
            downloaded_file = pathlib.Path(local_path)

            print(f"üìÅ Downloaded: {downloaded_file.name}")

            # Check file info
            file_info = check_h5_file_info(downloaded_file)

            if file_info:
                results.append(file_info)

                # Print summary for this file
                print(f"üìã File size: {file_info['file_size_mb']:.2f} MB")
                print(f"üìä Variables found: {len(file_info['variables'])}")

                # Look for temperature variables
                temp_vars = [name for name in file_info['variables'].keys()
                             if "Brightness Temperature" in name]

                if temp_vars:
                    print(f"üå°Ô∏è  Temperature variables: {len(temp_vars)}")
                    for var_name in temp_vars:
                        var_info = file_info['variables'][var_name]
                        print(f"   ‚Ä¢ {var_name}: shape={var_info['shape']}, "
                              f"valid={var_info['valid_pixels']}/{var_info['total_pixels']} "
                              f"({var_info['valid_percentage']:.1f}%)")
                else:
                    print("‚ö†Ô∏è  No temperature variables found!")

            # Clean up file
            try:
                downloaded_file.unlink()
                print(f"üóëÔ∏è  Cleaned up: {downloaded_file.name}")
            except:
                pass

        except Exception as e:
            print(f"‚ùå Error processing file {i + 1}: {e}")
            # Clean up on error
            if 'downloaded_file' in locals() and downloaded_file.exists():
                try:
                    downloaded_file.unlink()
                except:
                    pass

    return results


def analyze_size_variations(results: List[Dict]):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∞—Ä–∏–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏
    """
    print(f"\n{'=' * 60}")
    print("üìä SIZE VARIATION ANALYSIS")
    print(f"{'=' * 60}")

    if not results:
        print("‚ùå No results to analyze")
        return

    # Collect all temperature variable shapes
    temp_shapes = {}  # var_name -> [shapes]

    for result in results:
        for var_name, var_info in result['variables'].items():
            if "Brightness Temperature" in var_name:
                if var_name not in temp_shapes:
                    temp_shapes[var_name] = []
                temp_shapes[var_name].append(var_info['shape'])

    # Analyze each temperature variable
    for var_name, shapes in temp_shapes.items():
        print(f"\nüå°Ô∏è  Variable: {var_name}")
        print(f"üìè Found {len(shapes)} shapes:")

        # Count unique shapes
        unique_shapes = {}
        for shape in shapes:
            shape_str = f"{shape[0]}√ó{shape[1]}"
            if shape_str not in unique_shapes:
                unique_shapes[shape_str] = 0
            unique_shapes[shape_str] += 1

        # Print shape distribution
        for shape_str, count in sorted(unique_shapes.items()):
            percentage = count / len(shapes) * 100
            print(f"   ‚Ä¢ {shape_str}: {count} files ({percentage:.1f}%)")

        if len(unique_shapes) > 1:
            print(f"‚ö†Ô∏è  Size variation detected! {len(unique_shapes)} different sizes")

            # Analyze along-track dimension (first dimension) variation
            along_track_sizes = [shape[0] for shape in shapes]
            min_size = min(along_track_sizes)
            max_size = max(along_track_sizes)
            avg_size = sum(along_track_sizes) / len(along_track_sizes)

            print(f"üìê Along-track dimension:")
            print(f"   ‚Ä¢ Min: {min_size}")
            print(f"   ‚Ä¢ Max: {max_size}")
            print(f"   ‚Ä¢ Avg: {avg_size:.1f}")
            print(f"   ‚Ä¢ Variation: {max_size - min_size} pixels")
        else:
            print(f"‚úÖ All files have consistent size: {list(unique_shapes.keys())[0]}")


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ swaths
    """
    print("=== AMSR-2 Swath Size Checker ===\n")

    # Get time parameters
    start_input = input("Start datetime (YYYY-MM-DD HH:MM:SS, default: 2025-01-01 00:00:00): ").strip()
    if not start_input:
        start_input = "2025-01-01 00:00:00"

    end_input = input("End datetime (YYYY-MM-DD HH:MM:SS, default: 2025-01-01 02:00:00): ").strip()
    if not end_input:
        end_input = "2025-01-01 02:00:00"

    start_datetime = dt.datetime.strptime(start_input, "%Y-%m-%d %H:%M:%S").isoformat()
    end_datetime = dt.datetime.strptime(end_input, "%Y-%m-%d %H:%M:%S").isoformat()

    # Number of files to check
    max_files_input = input("Number of files to check (default: 10): ").strip()
    max_files = int(max_files_input) if max_files_input else 10

    print(f"\nüöÄ Checking swath sizes for period: {start_input} ‚Üí {end_input}")
    print(f"üìÅ Will check first {max_files} files")

    # Download and check files
    results = download_and_check_few_files(start_datetime, end_datetime, max_files)

    # Analyze results
    if results:
        analyze_size_variations(results)

        # Summary
        print(f"\n{'=' * 60}")
        print(f"‚úÖ SUMMARY: Checked {len(results)} files successfully")
        print(f"üîç This will help determine if size variations are:")
        print(f"   ‚Ä¢ Original in the satellite data")
        print(f"   ‚Ä¢ Or introduced during processing")
        print(f"{'=' * 60}")
    else:
        print("\n‚ùå No files were successfully processed")

    # Cleanup temp directory
    try:
        if TEMP_DIR.exists():
            for file in TEMP_DIR.glob('*'):
                if file.is_file():
                    file.unlink()
            TEMP_DIR.rmdir()
            print(f"üßπ Cleaned up temp directory")
    except:
        pass


if __name__ == "__main__":
    main()